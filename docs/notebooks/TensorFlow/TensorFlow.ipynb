{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9c1ec2-d11b-4a2c-8f3d-9c79df925b4c",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "E' una piattaforma open source per il machine learning. Fornisce supporto per computazioni numeriche che coinvolgono tensori, esecuzione dei modelli su GPU, differenziazione automatizzata ...\n",
    "Il nome della libreria significa 'flusso di tensori' e rappresenta il concetto di manipolazione di tensori che fluiscono attraverso un grafo computazionale (come una rete neurale profonda). Un tensore altro non è se non un array multidimensionale, una matrice estesa. Sono rappresentati da oggetti della classe Tensor e sono immutabili. Uno scalare (un numero) è un tensore di ordine 0, non sono presenti assi. Un vettore è un tensore di ordine 1 perché presenta un asse. Una matrice è un tensore di ordine 2 perché presenta 2 assi. In generale, un tensore di ordine $k$, avrà $k$ assi distinti e per identificare un suo elemento serviranno esattamente $k$ indici. Un tensore può essere visto in diversi modi come mostrato dalle seguenti immagini:\n",
    "\n",
    "![tensor_visualization](img\\tensor3_visual_60.png)\n",
    "\n",
    "![tensor_visualization](img\\tensor4_visual_60.png)\n",
    "\n",
    "Gli oggetti Tensor dovrebbero essere \"rettangolari\" cioè tutti gli elementi appartenenti ad uno stesso asse dovrebbero avere le stesse dimensioni. TensorFlow supporta altre tipologie di tensori che consentono forme diverse: Ragged Tensor e Sparse Tensor. Fra tensori, è possibile eseguire diverse operazioni come  la moltiplicazione (righe per colonne), l'addizione, la moltiplicazione per uno scalare, moltiplicazione elemento per elemento (prodotto nel senso di Hadamard). Gli elementi hanno di solito natura numerica intera o in virgola mobile (sono numeri interi o float) tuttavia possono contenere anche altre tipologie di dato come stringhe oppure numeri complessi (z = x + iy). \n",
    "TensorFlow supporta, come detto prima, tensori sparsi modellati come oggetti della classe SparseTensor. Questi permettono una maggiore efficienza sia nella loro memorizzazione, riducendo lo spazio necessario per mantenerli in memoria, sia nelle computazioni, consentendo una riduzione del tempo richiesto per eseguire operazioni su/fra di essi.\n",
    "\n",
    "### Grafi\n",
    "\n",
    "Le istruzioni TensorFlow vengono eseguite una alla volta dall'interprete Python secondo una modalita di esecuzione nota con il nome di \"eager execution\". TensorFlow supporta un'altra modalità di esecuzione ovvero la \"Graph execution\": prima di eseguire le operazioni fra tensori, viene costruito un grafo rappresentativo delle computazioni chiamato TensorFlow Graph e modellato dalla classe Graph. Un Graph è una struttura dati che contiene oggetti Operation, che rappresentano unità computazionali, ed oggetti Tensor, che rappresentano le unità di dati che fluiscono attraverso le Operations. Essendo delle strutture dati, gli oggetti Graph possono essere salvati, modificati e riportati ad un precedente stato. I grafi migliorano la portabilità dei modelli consentendo la loro esecuzione in ambienti privi di interprete Python come dispositivi mobili e dispositivi embedded. Inoltre, i grafi, sono facilmente ottimizzabili consentendo l'esecuzione di modelli più rapida, sfruttando il parallelismo e portabile, su diversi dispositivi, compresi i dispositivi con risorse limitate come dispositivi mobili e dispositivi embedded.\n",
    "\n",
    "Per creare ed eseguire un grafo si utilizza la funzione \"tf.function\" (dove tf rappresenta un alias per il namespace tensorflow) la quale prende in ingresso una Callable (cioè una funzione) e restituisce un oggetto di tipo PolymorphicFunction che a sua volta eredita da Callable. Una PolymorphicFunction modella una Callable Python che si occupa di costruire un Graph a partire da una funzione Python.\n",
    "L'utilizzo di grafi potrebbe rendere le computazioni più veloci. Tuttavia, sebbene la creazione del grafo possa essere costosa, tale costo viene ricompensato da un miglioramento delle prestazioni nelle computazioni successive.\n",
    "\n",
    "### Moduli\n",
    "\n",
    "I modelli sono generalmente composti da strati (layers). Uno strato rappresenta una funzione che esegue una qualche computazione matematica nota a priori agendo su tensori. Quindi una funzione da uno spazio tensoriale A verso uno spazio tensoriale B. Gli strati sono potenzialmente riusabili e costruiti al di sopra della classe base tf.Module.\n",
    "\n",
    "### Keras\n",
    "\n",
    "Sopra la libreria TensorFlow è possibile costruire una interfaccia di livello più alto. Ciò è stato fatto attraverso lo sviluppo della libreria Keras. In Keras uno strato è modellato attraverso l'interfaccia base Layer che eredita comunque da tf.Module (di tensorflow). E' possibile definire un modello utilizzando più componenti Layer annidati l'uno nell'altro. Oppure è possibile utilizzare l'interfaccia Model che a sua volta eredita da Layer. Model supporta l'addestramento, la valutazione il salvataggio ed altre utili attività rendendole più semplici."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e82083e-962c-4c91-86dd-24ce871eb4c3",
   "metadata": {},
   "source": [
    "### Ottimizzazione dei modelli in TensorFlow\n",
    "\n",
    "Per il processo di inferenza l'efficienza rappresenta un problema critico nel momento in cui il modello di machine learning viene deployato su di una qualche macchina a causa della latenza, dell'occupazione di memoria e del consumo di energia. In particolare quando il modello si vuole che sia messo in opera su dispositivi edge, come dispositivi mobili e dispositivi per l'IoT (Internet of Things o WoT - Web of Things). Qui le risorse computazionali, di memoria ed energetiche sono fortemente limitate il che complica o rende addirittura impossibile il deployment dei modelli su questi dispositivi. In queste situazioni la dimensione del modello e la sua efficienza inferenziale diventa una questione di primaria importanza.\n",
    "\n",
    "L'ottimizzazione di un modello di machine learning (o deep learning nel caso delle reti neurali) è sempre cosa gradita, a prescindere, perché porta a situazioni di vantaggio, di utilità. Possiamo citare alcuni di questi vantaggi: 1) riduzione della latenza nella fase di inferenza sia nel caso in cui il modello venga deployato nel cloud sia nel caso di deployment del modello in dispositivi edge (es. dispositivi mobili, Iot ...); 2) consente la possibilità di eseguire i modelli in dispositivi con risorse limitate nel senso spaziale (memoria), nel senso computazionale (basso troughput dell'unita di elaborazione), nel senso energetico (caso dei dispositivi alimentati usando accumulatori con capacità limitata); 3) ottimizzazione specifica per lo sfruttamento di particolari architetture harware per la computazione come GPU o TPU o altri acceleratori hardware\n",
    "\n",
    "Esistono e sono state studiate diverse tecniche di ottimizzazione per modelli di machine learning fra cui possiamo elencarne alcune come: 1) riduzione del numero totale dei paramentri di un modelli di apprendimento profondo attraverso pruning o potatura. Il modello risultante sarà cosiddetto sparsificato e la sparsificazione potrà essere strutturata o non strutturata a seconda del modo in cui i pesi nulli saranno distribuiti cioè se seguono una qualche struttura logica tipo la strutturazione N:M (cioè in ogni blocco di M parametri al più N saranno diversi da zero) oppure siano distribuiti in maniera completamente casuale. 2) riduzione della precisione numerica con la quale vengono espressi i parametri; 3) modifica della topologia della rete originale in modo da ottenerne un'altra che abbia meno paramentri e che sia più efficiente in fase di inferenza applicando metodi come la decomposizione tensoriale oppura la distillazione (? approfondire).\n",
    "\n",
    "TensorFlow supporta diverse tecniche di ottimizzazione quali la quantizzazione post-training, il quantization aware training, la sparsificazione mediante pruning e il clustering. In via sperimentale supporta anche l'ottimizzazione collaborativa che combina insieme diverse tecniche.\n",
    "I modelli quantizzati sono quelli in cui i pesi e le attivazioni vengono rappresentati utilizzando formati numerici con precisione più bassa rispetto a quella di riferimento (ad es. l'utilizzo di interi ad 8 bit anziché float a 32 bit). A volte, una rappresentazione a minore precisione diventa un requisito per poter sfruttare alcuni specifici hardware.\n",
    "I modelli sparsificati sono invece quelli in cui alcune connessioni fra neuroni di strati diversi vengono spezzate. Questo processo di potatura avviene azzerando alcuni pesi con conseguente sparisificazione dei rispettivi tensori.\n",
    "I modelli clusterizzati sono invece quelli in cui i paramentri dei modelli vengono sostituiti con un numero più piccolo di valori unici (i centroidi dei cluster individuati).\n",
    "La ottimizzazione collaborativa consente di applicare simultaneamente più tecniche di compressione per i modelli ed ottenere allo stesso tempo dei benefici in termini di accuratezza attraverso l'utilizzo della quantizzazione in fase di training (quantization aware training).\n",
    "\n",
    "La scelta delle tecniche di compressione da adottare dovrà essere frutto, necessariamente, di un compromesso come del resto accade ogni volta che ci si accinge a prende una decisione. Quando la compressione viene spinta oltre certi limiti l'accuratezza del modello subisce un degrado importante. Il modello compresso risulta non abbastanza \"intelligente\" da apprendere tutto ciò che sia necessario allo svolgimento del task in questione. Quindi anche se sarà richiesto meno spazio su disco e su memoria, meno computazioni per l'inferenza che sarà più veloce, meno consumo di energia, i risultati complessivi saranno per forza non adeguati a causa della mancanza di accuratezza dovuta alla limitata capacità di apprendimento. Se d'altro canto il modello non viene compresso a sufficienza, risulterà difficile eseguirlo su dispositivi con risorse limitate anche se poi sarà molto intelligente ed in grado di eseguire il task in questione in maniera eccellente. Il compromesso sarà dunque fra complessità del modello e la sua dimensione.\n",
    "\n",
    "TensorFlow mette a disposizione dei modelli pre-ottimizzati, da poter utilizzare così come sono, e dotati di certe prestazioni che potrebbero o no essere sufficienti per il nostro task. Nel caso in cui non si trovasse un modello che abbia prestazioni adeguate ai nostri casi d'uso è possibile comunque cercare di ottimizzare un modello già addestrato attraverso la quantizzazione post-training.\n",
    "\n",
    "Il TensorFlow Model Optimization Toolkit (MOT) è stato ampiamente utilizzato per convertire ed ottimizzare modelli TensorFlow in modelli TensorFlow Lite che risultano di più piccole dimensioni, più performanti nel senso di una minore latenza nell'inferenza e con una accuracy accettabile (la perdita in accuratezza rispetto al modello denso potrebbe essere minima o addirittura trascurabile). Queste condizioni rendono possibile l'esecuzione di reti neurali in dispositivi mobili o dispositivi IoT o, generalmente parlando, in tutti quei dispositivi in cui le risorse siano limitate.\n",
    "\n",
    "La sparsificazione di un modello può essere ottenuta durante la fase di training utilizzando il criterio basato sul valore assoluto dei pesi. I modelli sparsi sono più facili da comprimere e consentono, durante l'inferenza, di saltare tutte quelle computazioni che coinvolgano valori nulli migliorando così la latenza. Sono stati sperimentati miglioramenti fino a 6 volte relativamente all'occupazione di memoria di un modello sparsificato rispetto a quello denso di riferimento. \n",
    "\n",
    "Di seguito vengono riportati i risultati di alcuni test effettuati dagli autori. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec59e0-b273-485e-af76-3acf0c869b7a",
   "metadata": {},
   "source": [
    "![image_classification_results](img\\image_classification_results.jpg)\n",
    "\n",
    "![translation_results](img\\translation_results.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ab981-55c8-426d-88e6-4f502d937927",
   "metadata": {},
   "source": [
    "E' possibile notare, relativamente al task di classificazione di immagini, che una sparsificazione al 50% non strutturata consente un'accuratezza molto simile a quella del modello denso di riferimento ed in ogni caso più elevata rispetto a quella strutturata di tipo 2:4 (a cui corrisponde una sparsificazione sempre pari al 50%). Nel task relativo alla traduzione si può notare addirittura un aumento dell'accuratezza per sparsificazione all'80% ed una diminuzione trascurabile di essa per livelli di sparsificazione dall'85 al 90%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
