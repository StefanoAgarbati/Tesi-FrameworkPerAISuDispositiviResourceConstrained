{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7efd74af-401e-41b5-b8ce-0abef65c9948",
   "metadata": {},
   "source": [
    "* *tecniche di compressione dei modelli*\n",
    "    * servono a ridurre la dimensione, la complessità computazionale ed il consumo energetico di una rete neurale\n",
    "        * si cerca di mantenere le stesse prestazioni della rete di riferimento (quella non compressa)\n",
    "        * utilizzo più efficiente della memoria e dell'energia e riduzione dei tempi di computazione (training ed inferenza)\n",
    "        * il modello compresso può essere eseguito su dispositivi con risorse limitate\n",
    "            * smartphone, dispositivi edge ...\n",
    "    * l' autore individua sei tecniche di compressione\n",
    "        * *Down-sizing dei modelli*\n",
    "            * si creano modelli densi più piccoli per risolvere lo stesso problema\n",
    "            * tecniche di esempio\n",
    "                * distillazione del modello\n",
    "                * Neural Architecture Search\n",
    "                * consentono di trovare modelli densi più piccoli    \n",
    "        *  *Fattorizzazione degli operatori*\n",
    "            * porta ad una decomposizione degli operatori\n",
    "            * le matrici associate a strati densi vengono decomposte in matrici con rango ridotto\n",
    "            * matrici più piccole\n",
    "                * comportano meno operazioni\n",
    "                * comportano meno occupazione di memoria  \n",
    "        *  *Quantizzazione dei valori:*\n",
    "            * si riduce la precisione numerica dei valori nella rete (come i pesi, attivazioni o gradienti)\n",
    "            * i valori vengono codificati con sistemi che utilizzano un minor numero di bit\n",
    "            * diversi modi di codificare interi e numeri in virgola mobile che consentono un risparmio in termini di bit rispetto ai tipi di dati standard che prevedono 32 o 64 bit \n",
    "        *  *Compressione dei valori*\n",
    "            * consentono la compressione della struttura e dei valori (es. pesi)\n",
    "                * attraverso metodi basati sull'entropia o sulla perdita\n",
    "                * utilizzano la correlazione fra i valori \n",
    "        *  *Condivisione dei parametri*\n",
    "            * sfruttano la ridondanza dei parametri per comprimere un modello\n",
    "            * la ridondanza può essere agevolata durante il processo di training attraverso opportuni metodi\n",
    "            * (condivisione fa pensare al fatto che diversi parametri abbiano lo stesso valore e quindi possono essere condivisi in qualche modo)?\n",
    "        *  *Sparsificazione*\n",
    "            * riduce la complessità di un modello azzerando molti dei suoi parametri\n",
    "    * i metodi sopra elencati portano ad una riduzione dei requisiti spaziali (di memoria) e computazionali\n",
    "        * ad eccezione del metodo basato sulla condivisione che non è detto che comporti un miglioramento dei costi computazionali\n",
    "            * sebbene contribuisca comunque alla riduzione dimensionale del modello\n",
    "    * l'applicazione dei metodi non è esclusiva\n",
    "        * è possibile combinare insieme più metodi per ottimizzare la compressione (esistono dei lavori in letteratura al riguardo)\n",
    "* l'articolo focalizza sul metodo di sparsificazione noto anche come \"pruning\"\n",
    "    * metodo ritenuto molto efficace relativamente al task della compressione di un modello\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf35c2f6-f91e-4cc3-ae84-aefce2acfcca",
   "metadata": {},
   "source": [
    "I *modelli di deep learning*, anche noti come *reti di deep learning* o *reti neurali artificiali* possono essere visti come grafi composti da strati parametrizzabili, detti operatori, che insieme implementano una funzione complessa di tipo non lineare $f$. Consideriamo un apprendimento di tipo supervisionato dove abbiamo a disposizione un training set (esempi \"etichettati\") composto da un insieme di coppie $(\\mathbf{x, y})$ per $\\mathbf{x} \\in \\chi$ ed $\\mathbf{y} \\in \\mathcal{Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220bb2a4-c530-487c-a8d8-a4f563ba0534",
   "metadata": {},
   "source": [
    "+ l'obiettivo dell'apprendimento è quello di appunto apprendere una funzione $f \\colon \\chi \\to \\mathcal{Y}$\n",
    "    + la funzione $f$ è parametrizzata dai pesi $\\mathbf{w} \\in \\mathbb{R}^d$\n",
    "    + l'apprendimento consiste nel trovare un opportuno valore per il vettore dei parametri $\\mathbf{w}$\n",
    "        + in modo che dati un valore di input $\\mathbf{x} \\in \\chi$ ed un valore di output $\\mathbf{y} \\in \\mathcal{Y}$ deve aversi che $f(\\mathbf{x;w}) \\approx \\mathbf{y}$\n",
    "+ l'applicazione della funzione $f$ al vettore di input $\\mathbf{x}$ può essere interpretata come la trasformazione di $\\mathbf{x}$ strato per strato fino alla generazione dell'output $\\mathbf{y}$\n",
    "    + ogni strato trasoforma l'input in una nuova rappresentazione la quale poi viene presentata come input per lo strato successivo e così via\n",
    "    + per una rete addestrata questo processo di trasformazione prende il nome di *inferenza*\n",
    "    + durante la fase di training questo processo prende il nome di *forward pass*\n",
    "+ dato un problema possiamo provare a vedere se esista una rete neurale idonea alla sua soluzione\n",
    "    + questo processo di ricerca di una rete che sia capace di risolvere un dato problema può essere decomposto in due fasi\n",
    "        + *design*\n",
    "            + progettare una opportuna architettura per la rete\n",
    "                + dare alla rete una struttura appropriata alla soluzione del problema dato \n",
    "        + *training*\n",
    "            + dopo la fase di progettazione si esegue l'addestramento della rete\n",
    "            + obiettivo della fase di training è quello di trovare un valore ottimale per il vettore $\\mathbf{w}$ dei pesi\n",
    "+ l'architettura di una rete viene di solito progettata manualmente da un qualche architetto o gruppo di architetti\n",
    "    + la struttura risultante non viene poi modificata durante la fase di addestramento\n",
    "+ l'addestramento di una rete può essere un processo di tipo iterativo e quindi può essere ripetuto più e più volte\n",
    "    + per ricercare una, per così dire, \"ottimizzazione ottimale\"\n",
    "    + la qualità della trasformazione attuata dalla funzione $f(\\mathbf{x;w})$ (forward pass) viene misurata attraverso una loss function $\\mathscr{l} \\colon \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}$\n",
    "        + la loss function $\\mathscr{l}(\\mathbf{y}, f(\\mathbf{x;w}))$ fornisce una stima dell'accuratezza della precisione\n",
    "        + funzioni tipiche usate come loss function sono la norma $L_2$ oppure la cross-entropy\n",
    "    + al forward pass fa seguito la fase di backward pass\n",
    "        + in questa fase la perdita o errore si propaga all'indietro nella rete cioè dallo strato di output verso lo strato di input\n",
    "        + in corrispondenza di ogni strato il backward pass calcola il \"gradiente\" $g_k = \\partial \\ell / \\partial g_k$ ed applica una regola di appredimento (learning rule) per aggiornare i pesi $w_k$ in modo che la loss function $\\ell$ diminuisca\n",
    "            + questo viene fatto per ogni coppia $(\\mathbf{x, y})$ presente nell'insieme di training\n",
    "    + l'accuratezza ottenuta durante la fase di apprendimento viene misurata utilizzando un insieme di valori di $\\mathbf{x}$ mai visti prima\n",
    "        + tale insieme prende il nome di *test set*\n",
    "        + questa fase avrebbe lo scopo di valutare la capacità di generalizzazione del modello\n",
    "            + fare previsioni sull'output, a partire da input mai visti, che siano \"abbastanza\" vicine all'output reale\n",
    "            + esiste sempre il rischio di overfitting o sovra-adattamento\n",
    "                + il modello diventa bravissimo a prevedere solo ciò che ha già visto durante l'addestramento ma non è in grado di produrre previsioni corrette\n",
    "                    + cioè la probabilità che l'output sia \"molto errato\" (la distanza dal valore reale è grande) è elevata\n",
    "+ Consideriamo il caso del *percettrone multistrato* (Multilayer Perceptron o MLP)\n",
    "    + è un modelli di rete neurale artificiale composta da più strati (multilayer)\n",
    "    + ogni strato è connesso al successivo in accordo alla configurazione completamente connessa (fully connected)\n",
    "        + ogni neurone di uno strato è connesso a tutti i neuroni dello strato successivo\n",
    "            + più precisamente l'output di ogni neurone di uno strato viene portato in ingresso ad ogni neurone dello strato successivo \n",
    "            + escluso lo strato di uscita che non possiede uno strato successivo\n",
    "            + ma cosa è un neurone?\n",
    "                + è l'unità di base di una rete neurale artificiale la quale è composta da tanti neuroni connessi fra loro secondo qualche configurazione\n",
    "                + è un modello matematico molto semplificato del corrispondente neurone biologico\n",
    "                + .....\n",
    "                + **continua il discorso sul modello di McCulloch e Pitts**  \n",
    "    + ![Multilayer Perceptron structure](\\img\\MLP70.png)\n",
    "    + il modello ha uno strato di input $\\mathbf{x}_0$, due strati nascosti (hidden layer) $\\mathbf{x_1, x_2}$ ed uno strato di uscita $\\mathbf{x}_3$ con funzione di attivazione ReLU (rectified linear unit) definita come $\\sigma_R(x) = \\max(0, x)$\n",
    "    + il numero di neuroni presenti all'interno dello strato $i$ viene denotato con il simbolo $\\lvert x_i \\rvert$\n",
    "    + la fase forward pass può essere descritta nel seguente modo\n",
    "        + $f(\\mathbf{x_0;w}) = \\sigma_R(\\mathbf{w_3} \\cdot \\sigma_R(\\mathbf{w_2} \\cdot \\sigma_R(\\mathbf{w_1x_0 + b_1) + b_2) + b_3)}$ dove $\\mathbf{x_0}$ è il vettore di input, $\\mathbf{w_k}$ sono le matrici dei pesi associati allo strato $k$ mentre $\\mathbf{b_k}$ sono i bias asociati allo strato $k$-esimo\n",
    "        + ![same but spasified MLP network](\\img\\sparsifiedMLP60.png)\n",
    "        + sopra la stessa rete neurale ma sparsificata\n",
    "        + azzerando una riga di una matrice $\\mathbf{w}_k$ si elimina un neurone dallo strato $k$\n",
    "        + azzerando i valori all'interno di una colonna di una matrice $\\mathbf{w}_k$ si eliminano delle connessioni\n",
    "            + cioè delle caratteristiche di input\n",
    "+ l'addestramento di una rete neurale profonda può essere visto come il processo di minimizzazione di una data loss function utilizzando i dati presenti nel training set\n",
    "    + cioè si tratta di attuare un processo che porta a determinare quei valori dei pesi che rendono minima la funzione di perdita relativamente ai dati di addestramento\n",
    "        + pericolo di overfitting e bassa capacità di generalizzazione da parte del modello\n",
    "+ possiamo definire la funzione di perdita $L$ utilizzata durante il training come\n",
    "    + $L(\\mathbf{w}) = \\frac{1}{N} \\sum_{n=1}^N \\ell(\\mathbf{y}[n], f(\\mathbf{x}[n];\\mathbf{w})$\n",
    "    + quindi la loss function sarà una funzione $L \\colon \\mathbb{R}^d \\to \\mathbb{R}$ (dove $\\mathbf{w} \\in \\mathbb{R}^d$)\n",
    "+ il metodo di addestramento più utilizzato è quello basato sul gradiente discente stocastico (Stochastic Gradient Descent o SGD)\n",
    "    + (***si potrebbe fornire una breve descrizione di come funziona il metodo del gradiente discendente per ricavare una approssimazione del minimo di una data funzione***)\n",
    "    + le derivate parziali $\\partial L/\\partial w_k$ della funzione $L$ rispetto ai pesi $w_k$ possono essere calcolate, per un dato esempio $\\mathbf{x}$ usando un metodo di derivazione automatizzato\n",
    "    + ***si potrebbe accennare al funzionamento della differenziazione automatizzata***\n",
    "        + il metodo SGD richiede il calcolo del gradiente $\\nabla L$\n",
    "            + il gradiente ci dice qual'è la direzione di massima crescita di una funzione in un dato punto del suo dominio\n",
    "                + e dunque dal punto considerato ci si deve spostare un pochino (che dipende dall'iper-parametro noto come learning rate) in direzione opposta a quella del gradiente che rappresenta la direzione di minima crescita\n",
    "    + il gradiente calcolato viene poi utilizzato per applicare una regola di apprendimento (learning rule) $R$\n",
    "        + la regola definisce come aggiornare i pesi prima della successiva iterazione: $\\mathbf{w}^{(i+1)} = \\mathbf{w}^{(i)} + R(\\mathbf{g,w^{(i)}})$\n",
    "            + il nuovo peso viene calcolato a partire da quello vecchio, aggiungendo qualcosa (di positivo o negativo) ad esso calcolato secondo una specifica regola $R$\n",
    "        \n",
    "+ parliamo ora della matrice Jacobiana\n",
    "    + sia $F \\colon \\mathbb{R}^d \\to \\mathbb{R}^m$ una funzione di più variabili a valori vettoriali\n",
    "    + si tratta di una matrice i cui elementi sono rappresentati dalle derivate parziali prime della funzione $F$\n",
    "        + cioè ha per righe i gradienti delle componenti di $F$\n",
    "        + se $m = 1$ la matrice si riduce ad un vettore riga e corrisponde al gradiente vero e proprio\n",
    "        + possiamo perciò vedere la matrice Jacobiana come una generalizzazione del concetto di gradiente per funzioni a valori vettoriali\n",
    "        + nel nostro caso, data $L$ come funzione di perdita, la sua matrice Jacobiana è data da\n",
    "            + $\\mathbf{J} = \\nabla_\\mathbf{w} L = \\begin{bmatrix} \\frac{\\partial L}{\\partial w_1} & \\frac{\\partial L}{\\partial w_2} & \\cdots & \\frac{\\partial L}{\\partial w_d} \\end{bmatrix} = \\begin{bmatrix} g_1 & g_2 & \\cdots & g_d \\end{bmatrix}$\n",
    "\n",
    "+ parliamo adesso della matrice Hessiana\n",
    "    + la matrice Hessiana di una funzione di più variabili a valori in un campo scalare è una matrice quadrata $n \\times n$ i cui elementi sono rappresentati dalle derivate parziali seconde della funzione\n",
    "    + più formalmente, data la funzione $f \\colon \\mathbb{R}^n \\to \\mathbb{R}$, nell'ipotesi di esistenza di tutte le sue derivate parziali seconde, si definisce come matrice Hessiana della funzione $f$ la matrice $\\mathbf{H_f}(\\mathbf{x})$ data da\n",
    "$$\n",
    "    \\mathbf{H_f} = \n",
    "    \\begin{bmatrix}\n",
    "        \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2 } & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\      \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2 } & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "    + per una loss function $L$ differenziabile due volte la sua matrice Hessiana si può rappresentare come $\\mathbf{H} = \\nabla_\\mathbf{w}^2 L$\n",
    "    + la matrice Hessiana fornisce informazioni relative alla geometria locale ovvero alla curvatura di una funzione in un intorno di un punto $\\mathbf{w}$\n",
    "    + sfruttando il teorema di Taylor per funzioni di più variabili è possibile ottenere una espressione che approssima al secondo ordine una funzione in un intorno di raggio $\\delta$ di un punto dato $\\mathbf{w}$ utilizzando il gradiente e la matrice Hessiana\n",
    "    + possiamo quindi scrivere\n",
    "$$\n",
    "    L(\\mathbf{w} + \\delta \\mathbf{w}) \\approx L(\\mathbf{w}) + \\nabla_\\mathbf{w} L \\delta \\mathbf{w} + \\frac{1}{2} \\delta \\mathbf{w}^T \\mathbf{H} \\delta \\mathbf{w}.\n",
    "$$\n",
    "    + l'approssimazione sopra descritta prende anche il nome di modello quadratico locale per la loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc4d69-2cb5-4bc8-b09a-1d136ce99cf7",
   "metadata": {},
   "source": [
    "+ potremmo pensare che ad ogni esempio $\\mathbf{x}$ corrisponda una certa probabilità che la rete fornisca come risultato $\\mathbf{y}$\n",
    "+ siano dati un insieme di input $\\mathbf{x} \\in \\chi$ e un insieme di corrispondenti output $\\mathbf{y} \\in \\mathcal{Y}$\n",
    "    + ipotizziamo che i vettori di input $\\mathbf{x}$ siano estratti da una distribuzione $Q_\\mathbf{x}$ e che i corrispondenti valori di output $\\mathbf{y}$ siano presi da una distribuzione condizionale $Q_\\mathbf{y|x}$\n",
    "        + la distribuzione di probabilità congiunta può essere definita come $Q_\\mathbf{x,y} = Q_\\mathbf{x}Q_\\mathbf{y|x}$\n",
    "    + l'obiettivo è quello di minimizzare la distanza fra la probabilità congiunta target $Q_\\mathbf{x,y}$ e quella appresa $P_\\mathbf{x,y}(\\mathbf{w})$\n",
    "        + è comune usare come misura della distanza fra due distribuzioni di probabilitàla divergenza di Kullback-Leibler (indicata con KL)\n",
    "            + misura la distanza (non simmetrica) fra due distribuzioni di probabilità $P$ e $Q$\n",
    "            + si indica con il simbolo $D_{KL}(P\\lVert Q)$ che significa divergenza di $Q$ da $P$ secondo Kullback-Leibler\n",
    "    + addestrare il modello significa fare in modo che esso apprenda la distribuzione di probabilità condizionata $P_{y|x}(\\mathbf{w})$ che sia più \"vicina\" possibile alla reale distribuzione $Q_{y|x}$\n",
    "+ ***continua il discorso sulla prospettiva probabilistica dei modelli di ML***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d61dfc5-27d0-4efe-88e9-424077a99be1",
   "metadata": {},
   "source": [
    "### Panoramica sulla sparsificazione\n",
    "\n",
    "Perché sparsificare un modello? Principalmente per due ragioni: 1) per migliorare le prestazioni relativamente all'addestramento e all'inferenza; 2) per migliorare la capacità del modello di generalizzare (e migliorare la sua robustezza). (Aggiungerei anche miglioramenti legati al consumo energetico poiché, in generale, ad un minor costo computazionale corrisponde un minore consumo di energia). Bisogna osservare che la sparsificazione richiede un compromesso fra risparmio in termini computazionali e spaziali e accuratezza del modello. Una estrema sparsificazione porta con certezza ad una riduzione significativa dell'accuratezza del modello. Questo perché il ridotto numero di parametri non consente al modello di apprendere con profondità dai dati di training.\n",
    "La capacità di un modello di generalizzare rappresenta un aspetto molto importante nel deep learning e rappresenta una misura di quanto un modello riesca ad eseguire previsioni corrette su dati di input mai visti prima. ALcuni schemi di sparsificazione sembrano seguire la teoria del rasoio di Occam (la quale afferma che se per spiegare una fenomeno esistono diverse teorie ugualmente valide, è preferibile scegliere quella più semplice)\n",
    "\n",
    "![Accuracy Vs. Sparsity](\\img\\accuracyVsSparsity.png)\n",
    "\n",
    "Come si può osservare dalla figura sopra, all'aumentare della sparsità vi è, inizialmente un leggero aumento dell'accuratezza e delle prestazioni. Questo fenomeno può essere spiegato sulla base del fatto che in un modello più piccolo l'algoritmo di apprendimento focalizza maggiormente sugli aspetti più importanti e e generali (?). Aumentando ancora la sparsità, l'accuratezza, ad un certo punto raggiunge un valore massimo e le prestazioni migliorano ulterioremente. Ora aumentando ancora il livello di sparsità l'accuratezza si degrada molto velocemente mentre le prestazioni sembrano tendere asintoticamente ad un certo valore massimo. Da qui nasce la necessità di compromesso, come del resto avviene in ogni progetto ingegneristico.\n",
    "Risulta complesso il problema del raggiungimento delle migliori prestazioni possibili dato un specifico livello di sparsità. Esistono delle tecniche di archiviazione e di computazione che consentono di sfruttare al meglio la sparsità ma solo all'interno di un determinato intervallo o solo quando gli elementi non nulli siano distribuiti in specifici modi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e0217-bc77-47a8-aab7-afba1879e412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
