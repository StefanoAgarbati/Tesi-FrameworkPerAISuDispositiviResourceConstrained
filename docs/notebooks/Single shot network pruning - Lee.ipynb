{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0224ca-8a3e-4d06-9431-c9927b11b187",
   "metadata": {},
   "source": [
    "## Note sull'articolo SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY di Lee, Torr, Ajanthan - 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c466f-cd1d-47c0-b004-20d6fbd3e772",
   "metadata": {},
   "source": [
    "Le reti neurali profonde vengono utilizzate molto spesso nel machine learning. Tuttavia accade che esse siano sovraparametrizzate ovvero contengono più parametri di quelli necessari alla risoluzione di un dato problema. Le conseguenze di questo fatto si possono individuare nell'aumento del tempo di addestramento della rete, un aumento del tempo di inferenza di un output a partire da un dato input, un aumento dello spazio necessario per la memorizzazione della rete ed infine un aumento del consumo di energia (cioè aumento del costo computazionale, spaziale ed energetico). La possibilità di ridurre la dimensione del modello, nel senso del numero dei parametri, con perdite prestazionali minime, trascurabili, consentirebbe di affrontare le problematiche relative ai costi e di utilizzare tali reti in applicazioni real-time (che richiedono tempi di inferenza molto piccoli) e in tutti quei dispositivi con risorse limitate (resource-constrained). Le reti compresse, utilizzano le capacità del modello in modo più efficiente favorendo la generalizzazione e riducendo l'overfitting (cioè il modello prevede con precisione elevata gli input già visti nella fase di training ma non se la cava altrettanto bene con input nuovi e mai visti). La tecnica di riduzione della dimensionalità di una rete prende il nome di **pruning**. Quindi l'obiettivo principale del pruning è quello di ricavare una rete neurale più piccola da una rete di riferimento più grande (densa) ma che abbia le stesse prestazioni di quella di riferimento. La maggior parte delle tecniche esistenti parte da una rete neurale già addestrata per poi applicare alcuni criteri di ottimizzazione per ridurne iterativamente la dimensione. La natura iterativa del processo training-pruning è costosa e non facilmente generalizzabile (cioè applicabile ad altre architetture). \n",
    "Gli autori propongono un **metodo basato sulla salienza** (rilevanza) che consente di identificare le connessioni più rilevanti al problema da risolvere prima dell'addestramento vero e proprio del modello. Una connessione sarà considerata più o meno importante sulla base della sua influenza sulla loss function, influenza che prende il nome di **sensibilità della connessione** (connection sensitivity). Il **livello di sparsità** della rete viene definito a priori, dove per livello di sparsità si intende il numero di parametri non nulli che si vuole abbia la rete. Le connessioni ridondanti vengono \"*potate*\" una sola volta (single shot) prima della fase di addestramento vero e proprio. Dal pruning otteniamo una rete cosiddetta **sparsificata** che verrà addestrata in maniera classica (cioè usando i metodi classici di addestramento). \n",
    "Il metodo SNIP è stato valutato usando i dataset MNIST, CIFAR-10 e Tiny-ImageNet per problemi di classificazione ed utilizzando diverse architetture per la rete. Sono state ottenute reti molto sparse aventi praticamente lo stesso grado di accuratezza della rete di riferimento.\n",
    "\n",
    "I **metodi classici** di pruning delle reti neurali si possono suddividere in due gruppi:\n",
    "* **metodi basati sulla penalità**:\n",
    "    * all'interno della loss function vengono introdotti dei termini di penalità in modo tale che il processo di addestramento penalizzi l'importanza dei pesi. Dopodiché tutti quelli con un valore al di sotto di una certa soglia vengono eliminati (azzerati).\n",
    "* **metodi basati sulla salienza**:\n",
    "    * si definiscono dei criteri per valutare la rilevanza dei pesi. I criteri possono essere basati sul valore assoluto dei pesi oppure sulla matrice Hessiana associata alla loss function. Vengono mantenuti solo i k valori più grandi con k rappresentativo del livello di sparsità.\n",
    "Oltre alla tecnica del pruning esistono altre tecniche di compressione delle reti. Esiste un insieme di lavori relativi ai metodi di compressione della rappresentazione dei pesi come la quantizzazione, la precisione ridotta e la rappresentazione binaria.\n",
    "\n",
    "Le reti neurali sono generalmente sovraparametrizzate e attraverso il pruning è possibile ricavare una rete di dimensioni molto più piccole con prestazioni equiparabili a quelle della rete originale migliorandone al contempo la capacità di generalizzare e riducendone l'overfitting. Ne consegue che l'obiettivo del pruning è proprio quello di ottenere una rete neurale sparsificata con prestazioni virtualmente identiche a quelle della rete densa originale. La problematica del pruning può essere formalizzata attraverso un problema di ottimizzazione.\n",
    "\n",
    "Sia $\\mathcal{D} = \\{\\left(\\mathbf{x}_i, \\mathbf{y}_i\\right)\\}_{i=1}^n$ un dataset e sia $\\kappa$ il desiderato livello di sparsità ovvero il numero di pesi non nulli. Il pruning di una rete neurale può essere formalizzato attraverso il seguente problema di ottimizzazione:\n",
    "$$\n",
    "    \\min_{\\textbf{w}}\\mathit{L}\\left(\\mathbf{w};\\mathcal{D}\\right) = \\min_{\\textbf{w}}\\frac{1}{\\mathit{n}}\\sum_{i=1}^n\\mathit{l}\\left(\\mathbf{w};\\left(\n",
    "    \\mathbf{x}_i,\\mathbf{y}_i\\right)\\right),\n",
    "$$\n",
    "$$\n",
    "    \\text{dove} \\quad \\mathbf{w} \\in \\mathbb{R}^{m}, \\quad \\lVert\\mathbf{w}\\rVert_0 \\le \\kappa \\:.\n",
    "$$\n",
    "Qui $\\mathit{l}\\left( \\cdot \\right)$ rappresenta la loss function (e.g. cross-entropy) $\\mathbf{w}$ è l'insieme dei paramtri della rete neurale,$\\mathit{m}$ il numero totale di parametri mentre $\\lVert \\cdot \\rVert_0$ rappresenta la norma $\\mathit{L}_0$ (dato un vettore $\\mathit{x} = (\\mathit{x}_1,\\mathit{x}_2,\\ldots,\\mathit{x}_n)$ si ha che $\\lVert\\mathit{x}\\rVert_0 = \\lvert\\{\\mathit{i} \\in \\mathbb{N}:\\quad \n",
    "\\mathit{x}_i \\ne 0\\}\\rvert$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e058159-3785-4047-9164-8f3ec5f806e7",
   "metadata": {},
   "source": [
    "Per risolvere il problema di ottimizzazione di cui sopra si utilizza generalmente un approccio basato sulla penalità ovvero vengono aggiunti alla loss function degli iperparametri che penalizzano i pesi, imponendo un certo livello di sparsità. Per ottenere il valore ottimo di $\\mathbf{w}$ si può utilizzare il metodo del gradiente discendente. I metodi basati sulla salienza vengono ritenuti più efficaci rispetto a quelli basati sulla penalità sopratutto per ciò che riguarda il livello di sparsità ottenuto e le prestazioni del modello sparsificato.\n",
    "Il metodo basato sulla salienza tratta il problema di riduzione della dimensionalità di una rete in maniera diversa. I parametri ridondanti vengono rimossi direttamente in maniera selettiva sulla base di un criterio che definisca la loro salienza. Un criterio possibile è quello basato sul valore assoluto dei pesi e un limite superiore a tale valore assoluto. Quindi tutti i pesi il cui valore assoluto risulti essere inferiore ad un dato valore di soglia vengono considerati ridondanti e dunque eliminati (cioè azzerati). Un altro criterio utilizzato è basato sulla matrice Hessiana associata alla loss function:\n",
    "$$\n",
    "    \\mathit{s}_j = \\lvert \\mathit{w}_j \\rvert \\quad, \\text{per il criterio basato sul valore assoluto}\n",
    "$$\n",
    "$$\n",
    "    \\mathit{s}_j = \\frac{w_j H_{jj}}{2} \\quad, \\text{per il criterio basato sulla matrice Hessiana della loss function}\n",
    "$$\n",
    "Per la connessione $\\textit{j}$, il valore $\\textit{s}_j$ rappresenta il punteggio di salienza cioè la rilevanza associata al peso $\\textit{w}_j$, $\\textit{w}_j$ rappresenta il peso mentre $\\textit{H}_{jj}$ rappresenta il valore in posizione (j,j) della matrice Hessiana associata alla loss function nel punto $\\textit{w}_j$. Secondo quest'ultimo criterio più alto sarà il valore maggiore sarà l'importanza del peso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff609c27-2638-4c5a-8996-ee7e8528e439",
   "metadata": {},
   "source": [
    "Gli autori hanno elaborato un criterio che consente di determinare la rilevanza dei pesi in maniera dipendente dai dati ed una sola volta, prima della fase di addestramento vero e proprio che potrà essere poi effettuata direttamente sulla rete già sparsificata con guadagni in termini temporali, grazie anche alla possibilità di disporre di librerie software capaci di trattare in maniera efficiente matrici sparse.\n",
    "Si rende necessaria la definizione di un criterio che permetta di stabilire l'importanza di una connessione anche detta sensibilità (connection sensitivity). L'importanza (o sensibilità) di una connessione dovrebbe essere misurata in maniera indipendente dal suo peso. A tale scopo viene introdotta la variabile $\\mathbf{c} \\in \\{0,1\\}^m$ (ovvero l'insieme di tutte le sequenze binarie di m elementi) usata per rappresentare la connettività dei parametri $\\mathbf{w}$ (si associa una sequenza binaria c ad ogni valore del vettore w).\n",
    "Sia $\\kappa$ il livello di sparsità desiderato. Il problema di ottimizzazione può essere riformulato nel seguente modo:\n",
    "$$\n",
    "     \\min_{\\textbf{c,w}}\\mathit{L}\\left(\\mathbf{c}\\odot\\mathbf{w};\\mathcal{D}\\right) = \\min_{\\textbf{c,w}}\\frac{1}  {\\mathit{n}}\\sum_{i=1}^n\\mathit{l}\\left(\\mathbf{c}\\odot\\mathbf{w};\\left(\n",
    "    \\mathbf{x}_i,\\mathbf{y}_i\\right)\\right),\n",
    "$$\n",
    "$$\n",
    "    \\text{dove} \\quad \\mathbf{w} \\in \\mathbb{R}^{m}, \\quad \\mathbf{c} \\in \\{0,1\\}^m, \\quad \\lVert\\mathbf{c}\\rVert_0 \\le \\kappa \\:.\n",
    "$$\n",
    "dove $\\odot$ denota il prodotto di Hadamard (o prodotto di Schur o prodotto elemento per elemento che date due matrici $\\mathit{A}$ e $\\mathit{B}$ della stessa dimensione $\\mathit{m} \\times \\mathit{n}$ restituisce una matrice della stessa dimensione i cui elementi sono tali che\n",
    "$(\\mathit{A} \\odot \\mathit{B})_{ij} = (\\mathit{A})_{ij} (\\mathit{B})_{ij}$ - fonte Wikipedia Hadamard Product). La soluzione al problema di cui sopra è ancora più complessa di quella relativa al problema precedente a causa dell'introduzione della variabile $\\mathbf{c}$. In ogni caso, abbiamo separato il peso di una connessione $(\\mathbf{w})$ dal fatto che la connessione sia presente oppure no $(\\mathbf{c})$. Questo consente di valutare l'impatto di ogni connessione sulla loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38be74-c32f-41ab-b105-739efb531b84",
   "metadata": {},
   "source": [
    "Il valore di $\\mathit{c}_j$ indica se una connessione è attiva $(\\mathit{c}_j = 1)$ oppure se una connessione è stata \"potata\" $(\\mathit{c}_j = 0)$. Per comprendere l'effetto di una connessione sulla loss function potremmo valutare la stessa quando $\\mathit{c}_j = 1$ e poi quando $\\mathit{c}_j = 0$ lasciando tutto il resto inalterato. Vale a dire calcolare\n",
    "$$\n",
    "    \\Delta \\mathit{L}_j(\\mathbf{w};\\mathcal{D}) = \\mathit{L}(\\mathbf{1} \\odot \\mathbf{w};\\mathcal{D}) - \\mathit{L}((\\mathbf{1} - \\mathbf{e}_j) \\odot \\mathbf{w};\\mathcal{D})\n",
    "$$\n",
    "Il calcolo di $\\Delta \\mathit{L}_j$ risulta molto costoso. Inoltre la presenza della variabile binaria $\\mathbf{c}$ rende la loss function $\\mathit{L}$ non continua e quindi non differenziabile rispetto a $\\mathbf{c}$. Rilassando il vincolo di binarietà della variabile $\\mathbf{c}$ è possibile approssimare $\\Delta \\mathit{L}_j$ con la sua derivata parziale rispetto a $\\mathit{c}_j$ in un intorno $(1-\\delta,1+\\delta)$ di raggio $\\delta$ del punto $\\mathbf{c} = 1$\n",
    "Infatti, $\\partial\\mathit{L}$/$\\partial \\mathit{c}_j$ rappresenta una versione infinitesimale dell'incremento $\\Delta \\mathit{L}_j$ e misura la valocità con cui cambia $\\mathit{L}$ in direzione $\\mathit{c}_j$ per variazioni infinitesimali della variabile $\\mathbf{c}$ nell'intorno $(1-\\delta,1+\\delta)$. Questa derivata parziale può essere calcolata in maniera efficiente attraverso tecniche di differenziazione automatica per ogni $\\mathit{j}$. Possiamo anche vedere $\\partial\\mathit{L}$/$\\partial \\mathit{c}_j$ come misura della variazione della loss function in corrispondenza di perturbazioni infinitesime $\\delta$ del peso $\\mathit{w}_j$. Gli autori sono interessati a scoprire quali siano le connessioni importanti (sensibili) all'interno dell'architettura per poi poter eliminare tutte quelle non necessarie mediante pruning. A tale scopo hanno scelto come criterio di salienza per misurare la rilevanza di una connessione, il valore assoluto della derivata $\\mathit{g}_j$. Un valore assoluto grande significherebbe una importante variazione della loss function e di conseguenza la relativa connessione $\\mathit{c}_j$ verrebbe considerata importante e da preservare per apprendere il valore del peso $\\mathit{w}_j$. Sulla base di queste considerazioni si definisce come sensibilità di una connessione $\\mathit{s}_j$ il valore assoluto, normalizzato, della derivata parziale $\\mathit{g}_j = \\partial \\mathit{L}$/$\\partial \\mathit{c}_j$ ovvero\n",
    "$$\n",
    "    \\mathit{s}_j = \\frac{\\lvert \\mathit{g}_j(\\mathbf{w};\\mathcal{D})\\rvert}{\\sum_{k=1}^m \\lvert \\mathit{g}_k(\\mathbf{w};\\mathcal{D})\\rvert}\n",
    "$$\n",
    "Una volta calcolata la sensibilità $\\mathit{s}_j \\quad \\forall \\mathit{j}$ vengono mantenute solo le prime $\\kappa$ connessione più sensibili dove $\\kappa$ rappresenta il numero desiderato di pesi non nulli (o livello di sparsità). Per ottenere questo risultato la variabile $\\mathbf{c}$ viene impostata nel seguente modo\n",
    "$$\n",
    "    \\mathit{c}_j = \\mathbb{1}[\\mathit{s}_j - \\mathit{\\tilde{s}}_\\kappa \\ge 0], \\quad \\forall \\mathit{j} \\in \\{1, \\dots \\mathit{m}\\}\n",
    "$$\n",
    "dove $\\tilde{s}_\\kappa$ è il k-esimo elemento più grande del vettore $\\mathbf{s}$ mentre $\\mathbb{1}[\\cdot]$ è la funzione indicatrice (ritorna il valore 1 quando la condizione all'interno delle parentesi quadrate risulta vera e zero altrimenti)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623661d-5ba3-4cb5-835f-d836af3fb683",
   "metadata": {},
   "source": [
    "L'espressione della salienza dipende dai pesi, dal dataset e dalla loss function $\\mathit{L}$. I pesi iniziali vanno scelti con attenzione. Questo perché? Se i pesi sono troppo grandi, in valore assoluto, la funzione di attivazione risulterà piatta, satura e la sua derivata approssimativamente uguale a 0. Secondo il metodo del gradiente discendente i pesi vengono aggiornati secondo la formula \n",
    "$$\n",
    "    \\mathit{w}_j \\gets \\mathit{w}_j - \\alpha \\frac{\\partial \\mathit{L}}{\\partial \\mathit{w}_j}\n",
    "$$\n",
    "dove risulta, dalla regola della catena, che \n",
    "$$\n",
    "    \\frac{\\partial \\mathit{L}}{\\partial \\mathit{w}_j} = \\frac{\\partial \\mathit{L}}{\\partial \\mathit{a}} \\cdot \\frac{\\partial \\mathit{a}}{\\partial \\mathit{z}} \\cdot \\frac{\\partial \\mathit{z}}{\\partial \\mathit{w}} = \\delta \\cdot f'(z) \\cdot x_j\n",
    "$$\n",
    "e quindi\n",
    "$$\n",
    "     \\mathit{w}_j \\gets \\mathit{w}_j - \\alpha \\cdot \\delta \\cdot f'(z) \\cdot x_j\n",
    "$$\n",
    "Se $f'(z) \\approx 0$ il peso non si aggiornerà.\n",
    "Quindi i pesi iniziali dovrebbero appartenere ad un opportuno intervallo ed esistono dei metodi di inizializzazione per reti neurali."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55c55e-588b-47d0-882c-0b64491ea4f8",
   "metadata": {},
   "source": [
    "Si vuole che la misura di salienza sia indipendente da variazioni dell'architettura della rete (definibile attraverso una serie di paramatri come ad esempio il numero degli strati, il numero dei neuroni per ogni strato, la funzione di attivazione applicata ai vari neuroni ecc...). I pesi vengono generalmente inizializzati in maniera casuale a partire da una distribuzione normale (Gaussiana) .... (discorso sulla varianza non chiaro)\n",
    "Il metodo descritto dagli autori si basa sulla ricerca delle connessioni ridondanti utilizzando una misura di sensibilità. Questo consente di identificare e sottoporre a pruning tutte quelle connessioni ridondanti in un solo passaggio (single shot) prima del training della rete il quale potrà essere svolto direttamente sulla rete sparsificata. Gli autori hanno chiamato il loro metodo SNIP che sta per Single Shot Network Pruning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c94ee-dce3-46f8-9b61-4f376aee13c8",
   "metadata": {},
   "source": [
    "Sono stati eseguiti degli esperimenti relativi all'algoritmo SNIP usando i dataset MNIST, CIFAR-10 e Tiny-ImageNet usando diverse architetture e diversi livelli di sparsificazione. I risultati mostrano che è possibile ridurre il numero dei parametri della rete di riferimento (quella densa) fino al 99% (cioè il 99% dei parametri divengono nulli) con diminuzione dell'accuratezza trascurabili o minime rispetto a quella della rete di riferimento. Sono stati eseguiti dei confronti fra lo SNIP e altri metodi di pruning descritti da altri autori, la maggior parte dei quali richiede il pre-addestramento, l'introduzione di iperparametri risultando più laboriosi e costosi. Gli esperimenti mostrano addirittura un miglioramento dell'accuratezza rispetto alla rete di riferimento per alcuni livelli di sparsificazione (caso del 98% per le architetture LeNet). Il metodo SNIP sembra non costare quasi nulla poiché non richiede né pre-addestramento né iperparametri aggiuntivi. E' possibile ridurre significativamente il tempo necessario all'addestramento vero e proprio considerando solo i pesi sopravvissuti al processo di pruning precedente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136c257-1df2-45b4-aeac-ddcfa07840c2",
   "metadata": {},
   "source": [
    "Gli autori evidenziano l'applicabilità del loro metodo di pruning anche alle moderne architetture per reti neurali ovvero quelle convoluzionali, residuali e ricorrenti. Le esperienze mostrano che lo SNIP rende possibile raggiungere elevati livelli di sparsificazione (comprese fra il 90% e il 97%) in diverse moderne architetture con perdita minima o trascurabile dell'accuratezza (< 1%). Non è necessario modificare il metodo adattandolo a diverse architetture per reti neurali il che dimostra la sua versatilità e scalabilità a differenza di alcuni altri metodi di pruning esistenti. Il metodo funziona anche sulle reti neurali ricorrenti (RNN) le quali risultano difficili da sparsificare usando altri metodi l'applicazione dei quali comporta aumenti nell'errore percentuale non trascurabili.\n",
    "Non vi è tuttavia ancora chiarezza sul fatto che le connessioni eliminate siano effettivamente quelle non importanti. Gli autori forniscono una dimostrazione rivolta a provare che il metodo SNIP elimina solo quelle connessioni rilevanti alla soluzione del problema in questione."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
