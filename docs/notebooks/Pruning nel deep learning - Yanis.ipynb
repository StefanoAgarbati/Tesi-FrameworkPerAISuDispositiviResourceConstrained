{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf36ac7-4873-4ca6-b6b5-13434ae10c65",
   "metadata": {},
   "source": [
    "## Pruning in neural networks - Yanis Chaigneau - alcune note\n",
    "\n",
    "E' possibile rendere più efficiente, dal punto di vista computazionale, la \n",
    "moltiplicazione fra matrici, attraverso l'utilizzo di matrici in forma  \n",
    "sparsa.\n",
    "Parliamo di apprendimento profondo e reti neurali artificiali profonde. \n",
    "Queste sono generalmente dense, sovraparametrizzate (hanno molti più \n",
    "parametri di quelli effettivamente necessari per risolvere un dato problema \n",
    "ed è possibile dimostrare che l'abbondanza di parametri favorisce la \n",
    "convessità della loss function e favorisce la convergenza del metodo del \n",
    "gradiente discendente usato per la sua ottimizzazione). Si rende necessario,\n",
    "per motivi di efficienza, procedere alla potatura della rete neurale. E' \n",
    "possibile notare un parallelismo fra aumento delle prestazioni di CPU e GPU \n",
    "ed aumento delle dimensioni dei modelli. E' stata prevista una nuova legge \n",
    "di Moore, relativa alle dimensioni dei modelli di deep learning, la quale \n",
    "afferma che la dimensione dei modelli aumenta di un fattore dieci ogni anno. \n",
    "La crescita dei modelli segue una legge di tipo esponenziale nel numero degli\n",
    "anni. Questo mette in luce alcune problematiche. L'addestramento di tali\n",
    "modelli richiede una grande quantità di energia, per produrre la quale viene \n",
    "emessa nell'atmosfera una corrispondente grande quantità di CO2 (e questo non è \n",
    "un bene a fronte del problema del cambiamento climatico). Anche per questo motivo\n",
    "è cruciale una riduzione dei parametri dei modelli.\n",
    "Alcuni dei pesi (parametri) di una rete neurale addestrata risultano essere \n",
    "inutili al fine della produzione dell'output o comunque non comportano un impatto\n",
    "significativo sul suo valore. Ne consegue la possibilità di eliminare tutti quei \n",
    "pesi considerabili non significativi \n",
    "attraverso una tecnica nota come potatura o **pruning**. Più specificamente\n",
    "esistono dei metodi di pruning che consentono l'eliminazione dei pesi prima della\n",
    "fase di addestramento ed uno di questi il **Single-Shot Network Pruning** o\n",
    "brevemente **SNIP** (Lee, Ajanthan, Torr - 2019)(*SI POTREBBE DARE\n",
    "UN'OCCHIATA AL CORRISPONDENTE ARTICOLO*).\n",
    "\n",
    "Nel seguito si focalizzerà sul consumo energetico dei modelli sottoposti a pruning poiché la corrente ricerca non mostra miglioramenti significativi in  termini di tempo nelle computazioni. Per migliorare l'efficienza computazionale è necessario l'utilizzo di metodi di moltiplicazione fra matrici sparse. Si parlerà di Torch Sparse ed il suo utilizzo per l'addestramento di reti neurali dense. Parleremo poi della libreria cuSparse la quale consente di migliorare ulteriormente il tempo di computazione. \n",
    "\n",
    "### Pruning nel deep learning: l'esempio di SNIP (Single Shot Network Pruning)\n",
    "\n",
    "#### Tecniche di pruning\n",
    "\n",
    "Nel dominio del deep learning per pruning si intende una tecnica progettata per diminuire la dimensione di una rete neurale artificiale rimuovendo i pesi non necessari, selezionati usando un qualche criterio, pur mantenendo una accuratezza simile a quella della rete originale.\n",
    "\n",
    "La riduzione del numero dei parametri di una rete porta a delle conseguenze positive quali:\n",
    "* **minore occupazione di memoria:** meno parametri da memorizzare significa meno memoria necessaria per archiviare la rete\n",
    "* **diminuzione del costo computazionale di addestramento:** una rete con meno parametri comporta un tempo di addestramento minore rispetto ad      un'altra rete dotata di più parametri.\n",
    "* **diminuzione del costo computazionale di inferenza:** i percorsi computazionali che vanno dallo strato di input allo strato di output, divengono generalmente più corti, portando ad un minor numero di computazioni da eseguire e quindi ad un minor costo, in termini temporali, di inferenza di un output a partire da un dato input\n",
    "* **diminuzione del consumo energetico:** la riduzione della dimensione di una rete porta ad un risparmio di energia sia per la fase di addestramento sia per la fase di inferenza (in accordo con quanto descritto ai punti superiori)\n",
    "\n",
    "Dal 1989 e per 30 anni, la modalità principale di attuazione della tecnica di pruning consisteva nell'applicare iterativamente i seguenti passi:\n",
    "* si procedeva al training della rete\n",
    "* si individuava la rilevanza di ciascuna unità (usando un qualche criterio sistematico)\n",
    "* le unità meno rilevanti venivano eliminate dalla rete (è probabile che servisse una qualche soglia per capire quanta \"intelligenza\" eliminare)\n",
    "L'applicazione di questa tecnica permetteva di comprendere meglio quali unità fossero più importanti per descrivere la rete. Ad oggi è ancora difficile giustificare perché alcune architetture funzionano meglio di altre per determinati problemi. Non esiste un metodo sistematico di progettazione delle reti in base al problema da affrontare (quindi si procede per esperienze, per tentativi (?)).\n",
    "Intorno all'anno 2010 è molto cresciuto l'interesse per le tecniche di pruning.\n",
    "\n",
    "#### Tecnica SNIP: pruning eseguito prima del training\n",
    "\n",
    "Nel 2019, un gruppo di ricercatori (Lee, Torr, Ajanthan) hanno ideato un metodo innovativo di pruning di un modello da eseguire prima dell'addestramento vero e proprio. La precisione della rete risultante dopo l'operazione di pruning è equivalente a quella della rete densa (cioè non soggetta a pruning). Il pruning viene eseguito una sola volta, all'inizio, prima del training vero e proprio. Così si evitano lunghi e costosi cicli di training-pruning e l'aggiunta e fine tuning di iperparametri. L'operazione consente di rimuovere fino al 98% dei parametri in una sola iterazione. Successivamente sarà possibile addestrare la rete usando i metodi classici e con notevole risparmio di tempo in termini computazionali, sia per addestramento che per inferenza di un output, che di spazio richiesto per l'archiviazione della rete. Gli autori \n",
    "Tuttavia, l'autore dell'articolo (Yanis...), facendo alcuni esperimenti, non ha potuto osservare miglioramenti significativi nel tempo di addestramento e nel consumo energetico fra la rete densa e la rete sparsificata mediante pruning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc369d-a33a-4ea0-a08a-8a6e4d4bd3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
