{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6493bac2-0a77-4afe-8cd8-7cb13f592259",
   "metadata": {},
   "source": [
    "## Accelerating inference with sparsity using NVIDIA Ampere architecure and NVIDIA TensorRT - NVIDIA Blog 2021\n",
    "\n",
    "Quando si sviluppano reti neurali è di utilità pensare a come rendere tali reti più veloci e più piccole nel senso della dimensione spaziale. Una rete più efficiente potrebbe fare previsioni migliori in meno tempo ed essere più adatta al deployment su ambienti vincolati cioè dispositivi con risorse limitate. La sparsificazione è una delle tecniche che potrebbe portare ad una riduzione del tempo di inferenza e ad una riduzione delle dimensioni della rete. Se i tensori dei pesi sono sparsificati cioè composti da molti valori nulli, questi non vanno considerati nei calcoli e non è necessario memorizzarli con conseguenti benefici temporali e spaziali. Tuttavia vi sono alcune problematiche:\n",
    "+ Accelerazione: se i valori nulli, all'interno dei tensori, sono distribuiti casualmente, senza una struttura logica particolare (in tal caso si parlerà di sparsificazione non strutturata), non sarà possibile sfruttare tutte quelle operazioni fra matrici e vettori supportate direttamente dall'hardware.\n",
    "+ Accuratezza: il processo di sparsificazione di una rete può portare ad un miglioramento del tempo di inferenza ma allo stesso tempo produce un peggioramento nell'accuratezza della rete stessa.\n",
    "+ Workflow: la maggior parte delle ricerche correnti mostrano che dato modello A è possibile raggiungere un grado di sparsificazione X. Il problema sorge quando si tenta di applicare il grado di sparsificazione X ad un altro modello B. La sparsificazione potrebbe non funzionare a causa della diversa architettura della rete, del task da risolvere o qualsiasi altro iperparametro.\n",
    "\n",
    "L'architettura Ampere per GPU NVIDIA indirizza queste problematiche. La libreria TensorRT, dalla versione 8.0, introduce supporto all'utilizzo degli Sparse Tensor Cores disponibili sulle architetture Ampere. (Cosa è e come è fatto e come funziona uno Sparse Tensor Core?). Lo sfruttamento degli Sparse Tensor Cores consente di ignorare i calcoli non necessari (ovvero quelli che coinvolgono valori dei pesi nulli) portando ad un aumento anche superiore al 30% del rapporto prestazioni/potenza(W) rispetto alle reti neurali dense\n",
    "\n",
    "Gli Sparse Tensor Cores sono in grado di accelerare calcoli che coinvolgano tensori sparsificati secondo lo schema strutturato 2:4 cioè per ogni blocco contiguo di 4 valori due di essi devono essere nulli. Questo porta ad una sparsità del 50%. Un tensore conforme a questo pattern è facile da comprimere.\n",
    "\n",
    "![matrix 2:4](img\\matrix_2_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb9cd6-ac5a-4cd7-89c3-03cddc2db82b",
   "metadata": {},
   "source": [
    "La matrice compressa, come rappresentato nella figura sopra, ha R righe e C/2 colonne ed è accompagnata da una matrice Rx(C/2) composta da indici rappresentati usando 2 bit ciascuno. Gli Sparse Tensor Cores sono in grado di accelerare i calcoli relativi a tensori aventi questo formato compresso considerando solo i valori non nulli. Utilizzano la matrice di indice (detta anche matrice dei metadati) per selezionare soltanto i valori che non siano nulli. Per sparsità al 50% possono eseguire i calcoli in metà tempo.\n",
    "\n",
    "![](img\\sparseTensorPerformance.png)\n",
    "\n",
    "Come mostrato dalla tabella, la sparsità porta ad una maggiore velocità delle operazioni indipendentemente dalla tipologia di dato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a796d656-d9ce-4d99-916a-b0ca1de2032e",
   "metadata": {},
   "source": [
    "La sparsità 2:4 consente di mantenere l'accuratezza rispetto al modello denso. Un modello molto veloce sarebbe del tutto inutile se non fosse accurato. E' stato quindi sviluppato un processo che consente di generare una sparsità strutturata di tipo 2:4 su di una rete neurale e che porti ad una accuratezza uguale a quella del modello denso:\n",
    "+ si parte da una rete densa addestrata che abbia raggiunto un ottimo livello di accuratezza\n",
    "+ si passa ad una fase di potatura dei pesi che porti a soddisfare il pattern di sparsità strutturato di tipo 2:4. Ogni quattro elementi ne rimuoviamo esattamente due.\n",
    "+ poi procediamo al riaddestramento della rete\n",
    "Nella fase di pruning bisogna decidere quali valori mantenere e quali valori porre a zero. Serve dunque un criterio. Quello più semplice è basato sul valore assoluto dei pesi (weight magnitude): poniamo a zero i valori che sono più prossimi ad esso. A questo punto sorge un problema. Impostando a zero la metà dei pesi di una rete neurale si avranno ripercussioni sull'accuratezza in fase di inferenza. Ecco giustificato l'ultimo passo del processo precedente che consiste in una ulteriore fase di addestramento. Questa servirà ad aggiornare i pesi rimasti quel tanto che sia sufficiente a recuperare l'accuratezza perduta successivamente alla fase di pruning.\n",
    "\n",
    "![accuracy_2_4_table](img\\accuracy2_4.png)\n",
    "\n",
    "### Sommario\n",
    "\n",
    "La sparsificazione è una tecnica popolare per comprimere reti neurali. Tuttavia, un grado di sparsità elevato, non consente di ottenere quei guadagni di prestazioni ed accuratezza promessi dalla ricerca. I ricercatori NVIDIA, hanno ideato il pattern di sparsificazione strutturata di tipo 2:4 ed implementato un supporto ad esso direttamente negli Sparse Tensor Cores dell'architettura NVIDIA Ampere. Questo consente di generare reti neurali sparsificate senza alcuna perdita di accuratezza rispetto al modello denso e successivamente TensorRT si occuperà dell'accelerazione in termini computazionali. Il risultato sarà un rete neurale più piccola, compressa, con necessità di memoria inferiori, più veloce in fase di inferenza grazie alle ottimizzazioni incapsulate direttamente all'interno dell'hardware ed accurata tanto quanto la rete neurale densa da cui essa deriva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17e6a3-7b4b-452f-b198-f28a79126981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
