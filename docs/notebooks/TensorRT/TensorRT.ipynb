{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe5384b3-244b-42b5-bbc0-a9fdd34e8ad7",
   "metadata": {},
   "source": [
    "## TensorRT\n",
    "\n",
    "TensorRT è un SDK (Software Development Kit) sviluppato da NVIDIA che consente di ottimizzare un modello di deep learning già addestrato per eseguire inferenze ad elevate prestazioni. TensorRT comprende un ottimizzatore per modelli di deep learning (reti neurali) (noto anche come optimizer) ed un runtime per l'esecuzione dei modelli ottimizzati su GPU NVIDIA.\n",
    "\n",
    "Può portare ad un miglioramento delle prestazioni computazionali di inferenza fino a 36 volte (36x) rispetto alla stessa ma eseguita su CPU.\n",
    "TensorRT è costruito al di sopra di NVIDIA CUDA ovvero un'architettura hardware per l'elaborazione parallela ed un ambiente di sviluppo per lo sfruttamento di tale architettura che consente di sfruttare le GPU non solo per la grafica ma anche per l'esecuzione di programmi generalmente eseguiti all'interno di CPU. TensorRT consente di prendere un modello descritto e pre-addestrato usando uno qualsiasi dei framework disponibili per poi ottimizzarlo attraverso diverse tecniche fra cui quantizzazione e sparsificazione ed eseguirlo su GPU NVIDIA per inferenza ad elevate prestazioni (bassa latenza ed elevato troughtput). Il modello ottimizzato può essere utilizzato per applicazioni real-time nei domini dell'automotive, dell' IoT e robotica.\n",
    "I test mostrano miglioramenti nell'inferenza relativa ad LLM da 4 ad 8 volte rispetto al riferimento, dove non si fa uso dell'ottimizzazione.\n",
    "Le ottimizzazioni portano anche a costi inferiori di mantenimento e minori consumi energetici da 3 a 5 volte rispetto al riferimento. Il consumo energetico ha assunto un'importanza cruciale per contrastare o limitare le conseguenze negative del già in atto cambiamento climatico contribuendo alla riduzione della produzione di CO2.\n",
    "\n",
    "TensorRT fornisce API sia per il linguaggio C++ sia per il linguaggio Python. L'API Python supporta l'interoperabilità con altre librerie come numpy. L'interfaccia per C++ può essere più efficiente rispetto a quella per Python e quindi consigliabile per applicazioni con requisiti più stringenti come ad esempio quelli relativi alle applicazioni nel dominio dell'automotive.\n",
    "\n",
    "TensorRT opera in due fasi. Nella prima fase, si fornisce a TensorRT la descrizione di um modello il quale viene ottimizzato per una specifica GPU. La seconda fase consiste nell'esecuzione del modello ottimizzato per generare inferenze (o output a partire da certi dati di input).\n",
    "Viene messa a disposizione un'interfaccia di alto livello chiamata Builder. Un Builder si occupa di ottimizzare un modello restituendo, alla fine del processo di ottimizzazione, un Engine. Quindi per costruire un Engine si deve: 1) progettare l'architettura di una rete ed esprimerla usando qualche linguaggio; 2) si deve specificare una configurazione per il Builder; 3) si chiama il Builder per costruire l'Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05412deb-8c39-4b86-a61a-63ddf14387f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c4460d-385b-4fd1-b89f-537afbfe1f68",
   "metadata": {},
   "source": [
    "Tutti i flussi di lavoro coinvolgono la conversione di un modello esistente in una sua rappresentazione fortemente ottimizzata. In TensorRT, tale rappresentazione viene chiamata *Engine*. Il processo base di conversione e deployment di un modello richiede l'esecuzione di una serie di passi:\n",
    "1. Export del modello: si parte sempre da un modello esistente addestrato utilizzando librerie come PyTorch, TensorFlow o ONNX\n",
    "1. Selezionare una precisione numerica: tensorRT può ottimizzare i modelli usando diverse rappresentazioni per i numeri, ciascuna con un diverso grado di precisione. Rappresentazioni possibili sono FP32, FP16, INT8. Il compromesso è fra accuratezza, velocità di esecuzione ed occupazione di memoria.\n",
    "1. Conversione del modello: TensorRT costruisce l'Engine applicando una serie di ottimizzazioni. Produce un file .plan o .engine che rappresenta l'engine ottimizzato per l'esecuzione su di una GPU con la quantizzazione scelta\n",
    "1. Deployment del modello: l'engine ottimizzato può essere caricato ed utilizzato per inferenza su diverse piattaforme\n",
    "\n",
    "Ci sono diversi modi per arrivare alla creazione di un modello ottimizzato usando TensorRT.\n",
    "1. Utilizzare Torch-TensorRT\n",
    "2. Utilizzare la conversione automatica a partire da un modello espresso in formato ONNX (Open Neural Network Exchange)\n",
    "3. Costruire una rete neurale manualmente utilizzando le API TensorRT disponibili per i linguaggi C++ e Python\n",
    "\n",
    "Torch-TensorRT è un inference compiler che a partire da un modello PyTorch ne crea una versione ottimizzata per GPU NVIDIA utilizzando la libreria TensorRT. Una versione più performante di conversione automatica è quella che sfrutta il formato ONNX. Una rete esistente può essere trasformata nel formato intermedio ONNX per poi poter essere ottimizzata automaticamente attraverso TensorRT. E' possibile infine costruire manualmente la rete neurale target utilizzando direttamente le API TensorRT. Quindi sarà sufficiente esportare solamente i pesi dal framework utilizzato per l'addestramento per poi caricarli nella rete costruita attraverso TensorRT.\n",
    "\n",
    "Il deployment di un modello ottimizzato usando TensorRT può avvenire in diversi modi:\n",
    "+ all'interno dell'ecosistema PyTorch\n",
    "+ all'interno dell'ecosistema NVIDIA usando l'API TensorRT\n",
    "+ utilizzando NVIDIA Triton Inference Server\n",
    "Quando si utilizza Torch-TensorRT, l'opzione più semplice è quella di eseguire il deployment all'interno dell'ecosistema PyTorch. I modelli Torch-TRT possono essere eseguiti come se fossero modelli PyTorch classici.\n",
    "Il flusso più comune per il deployment di un modello all'interno dell'ecosistema TensorRT è quello che parte dall'export di un modello, costruito ed addestrato usando un altro framework, in formato ONNX. (**Triton inference server......**)\n",
    "ONNX è un formato aperto progettato per rappresentare modelli di machine learning. E' possibile descrivere una NN utilizzando un qualsiasi framework e poi convertire il modello creato nel formato ONNX che risulterà indipendente dal framework utilizzato per la costruzione del modello. Successivamente sarà possibile importare il modello ONNX in un qualsiasi runtime (comunque supportato) per consentire la sua esecuzione. TensorRT mette a disposizione il parser, ONNX-TRT Parser, attraverso il quale si può convertire un modello ONNX in un engine TRT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5b714-48e9-4844-a21b-99888ddaa036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
